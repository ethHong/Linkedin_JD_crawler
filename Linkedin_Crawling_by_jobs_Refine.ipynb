{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains  \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm.notebook import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 1: job 별\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_user_agent():\n",
    "    ua = UserAgent()\n",
    "    \n",
    "    working = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\"\n",
    "    working_tail = \"(\" + working.split(\"(\")[-1]\n",
    "    random_head = ua.random.split(\"(\")[0]+\"(\"+ua.random.split(\"(\")[1]\n",
    "    return random_head + working_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_user_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID (Email)sukhyun9673@gmail.com\n",
      "PASSWORDsh96699669\n"
     ]
    }
   ],
   "source": [
    "ID = input(\"ID (Email)\")\n",
    "PASS = input(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HongSukhyun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: use options instead of chrome_options\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "userAgent = mock_user_agent()\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "#driver =  webdriver.Chrome(\"/Users/HongSukhyun/Desktop/SukhyunHong/20-2/UDS/Course_Recommendation/chromedriver\")\n",
    "#driverpath = os.getcwd()+\"/chromedriver_win\"\n",
    "\n",
    "driverpath = os.getcwd()+\"/chromedriver\"\n",
    "driver =  webdriver.Chrome(driverpath,  chrome_options=options)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "def check_http_error():\n",
    "    return \"HTTP ERROR 429\" in driver.page_source\n",
    "\n",
    "def Login_linkedin(driver, ID, PASS):\n",
    "    url = \"https://www.linkedin.com/\"\n",
    "    \n",
    "    driver.get(url)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "    #driver.find_element_by_xpath('/html/body/div/main/p/a').click()\n",
    "    \n",
    "    ID = ID\n",
    "    PASS = PASS\n",
    "    \n",
    "    try:\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"session_key\"]')\n",
    "        elem.send_keys(ID)\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"session_password\"]')\n",
    "        elem.send_keys(PASS)\n",
    "\n",
    "\n",
    "        driver.find_element_by_xpath('/html/body/main/section[1]/div[2]/form/button').click()\n",
    "    except:\n",
    "        if check_http_error() == True:\n",
    "            backup_df = df\n",
    "            print (\"Take a break...for 3 minuits...\")\n",
    "            time.sleep(180)\n",
    "            \n",
    "            elem = driver.find_element_by_xpath('//*[@id=\"session_key\"]')\n",
    "            elem.send_keys(ID)\n",
    "            elem = driver.find_element_by_xpath('//*[@id=\"session_password\"]')\n",
    "            elem.send_keys(PASS)\n",
    "\n",
    "\n",
    "            driver.find_element_by_xpath('/html/body/main/section[1]/div[2]/form/button').click()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_link(continue_link):\n",
    "    userAgent = mock_user_agent()\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    driverpath = os.getcwd()+\"/chromedriver\"\n",
    "    driver =  webdriver.Chrome(driverpath,  chrome_options=options)\n",
    "    \n",
    "    Login_linkedin(driver, ID, PASS)\n",
    "    \n",
    "    driver.get(continue_link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Login_linkedin(driver, ID, PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put your job position: data\n"
     ]
    }
   ],
   "source": [
    "job = input(\"Put your job position: \")\n",
    "region = \"대한민국\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"https://www.linkedin.com/jobs/search/?geoId=105149562&keywords=\"\n",
    "\n",
    "def refine(c):\n",
    "    c_ref = \"-\".join(c.split(\" \")).lower()\n",
    "    return c_ref\n",
    "\n",
    "link = header + refine(job)\n",
    "driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait 5 sec...\n"
     ]
    }
   ],
   "source": [
    "print(\"wait 5 sec...\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\") \n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_page = int(pages[-1].text.strip()) # # of total page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_source_pages():\n",
    "    time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "    \n",
    "    try:\n",
    "        p = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")\n",
    "    except:\n",
    "        driver.get(driver.current_url)\n",
    "        time.sleep(5)\n",
    "        p = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")\n",
    "    return [p, soup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_jd():\n",
    "    time.sleep(2)\n",
    "    soup = refresh_source_pages()[1]\n",
    "\n",
    "    potision = []\n",
    "    job_details = []\n",
    "\n",
    "    jobs = soup.find_all(\"li\", {\"class\": \"jobs-search-results__list-item occludable-update p0 relative ember-view\"})\n",
    "    jobs_id = [i[\"id\"] for i in jobs]\n",
    "    \n",
    "    for i in jobs_id:\n",
    "        driver.find_element_by_xpath('//*[@id=\"{}\"]'.format(i)).click()\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "        #refresh page source\n",
    "        soup = refresh_source_pages()[1]\n",
    "        \n",
    "        driver.implicitly_wait(10)\n",
    "        \n",
    "        Position =soup.find(\"h2\", {\"class\": \"jobs-details-top-card__job-title t-20 t-black t-normal\"}).text.rstrip()\n",
    "\n",
    "        Job_Details = soup.find(\"div\", {\"id\":\"job-details\"}).text.strip()\n",
    "        potision.append(Position)\n",
    "        job_details.append(Job_Details)\n",
    "\n",
    "    \n",
    "    return pd.DataFrame({\"Position\" : potision, \"Job_Details\": job_details})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many = 3 #안전하게 3번마다 Refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = refresh_source_pages()[0]\n",
    "soup = refresh_source_pages()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Page 넣으면 알아서 크롤링하게 만들기\n",
    "def crawl_job_description(starting_page, how_many, start_url):\n",
    "    if how_many>total_page:\n",
    "        how_many= total_page\n",
    "        \n",
    "    driver.get(start_url)\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(2)\n",
    "    pages = refresh_source_pages()[0]\n",
    "    soup = refresh_source_pages()[1]\n",
    "    \n",
    "    current = starting_page\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in trange(starting_page-1, how_many+starting_page-1):\n",
    "    \n",
    "        print (\"Crawling {} out of {} pages...\".format(current, total_page))\n",
    "\n",
    "        pages_meta = [j.text.strip().split()[0] for j in pages]    \n",
    "\n",
    "        #Do Crawling#\n",
    "        \n",
    "        crawed_page = crawl_jd()\n",
    "        df = pd.concat([df, crawed_page])\n",
    "        current = current+1\n",
    "        \n",
    "        if current>total_page:\n",
    "            break #Don't move page if it's last page \n",
    "        #Move page \n",
    "        \n",
    "        try:\n",
    "            index_of_next_page = pages_meta.index(str(i+2))\n",
    "        except ValueError:\n",
    "            index_of_next_page = len(pages_meta) -1 - pages_meta[::-1].index('…')\n",
    "\n",
    "        button_aria_label = pages[index_of_next_page].find(\"button\")[\"aria-label\"]\n",
    "\n",
    "        #해당 버튼이 나올때까지 기다려주기\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//*[@aria-label=\"{}\"]'.format(button_aria_label)).click()\n",
    "\n",
    "        except:\n",
    "            driver.get(driver.current_url)\n",
    "            driver.implicitly_wait(10)\n",
    "            button_aria_label = str(int(button_aria_label.split()[0])+1) + \" \" + button_aria_label.split()[1]\n",
    "            driver.find_element_by_xpath('//*[@aria-label=\"{}\"]'.format(button_aria_label)).click()\n",
    "\n",
    "        \n",
    "        driver.implicitly_wait(10)\n",
    "        print (\"Upcoming page is {}\".format(i+2))\n",
    "        upcoming = driver.current_url\n",
    "        #Refresh List\n",
    "        try:\n",
    "            pages = refresh_source_pages()[0]\n",
    "        except:\n",
    "            driver.get(driver.current_url)\n",
    "            time.sleep(3)\n",
    "            pages = refresh_source_pages()[0]\n",
    "        \n",
    "    return (df, upcoming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ed16c835f34ed6ba1545b0edde03b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4096a75bf74dc391ae4a25e90c8bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1 out of 40 pages...\n",
      "Upcoming page is 2\n",
      "Crawling 2 out of 40 pages...\n",
      "Upcoming page is 3\n",
      "Crawling 3 out of 40 pages...\n",
      "Upcoming page is 4\n",
      "\n",
      "Count: 21\n",
      "Refreshing for 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HongSukhyun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: use options instead of chrome_options\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188aa82d11d746b69fecdf6980918d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1 out of 40 pages...\n",
      "Upcoming page is 2\n",
      "Crawling 2 out of 40 pages...\n",
      "Upcoming page is 3\n",
      "Crawling 3 out of 40 pages...\n",
      "Upcoming page is 4\n",
      "\n",
      "Count: 42\n",
      "Refreshing for 1 times\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a9060280ab4d4e9d0b9151f0e89f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1 out of 40 pages...\n",
      "Upcoming page is 2\n",
      "Crawling 2 out of 40 pages...\n",
      "Upcoming page is 3\n",
      "Crawling 3 out of 40 pages...\n",
      "Upcoming page is 4\n",
      "\n",
      "Count: 63\n",
      "Refreshing for 1 times\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cc7db14c9149399e41c9cd8e225438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1 out of 40 pages...\n",
      "Upcoming page is 2\n",
      "Crawling 2 out of 40 pages...\n",
      "Upcoming page is 3\n",
      "Crawling 3 out of 40 pages...\n",
      "Upcoming page is 4\n",
      "\n",
      "Count: 84\n",
      "Refreshing for 1 times\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0add9b859a3b4d41ae58e43037f5d9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1 out of 40 pages...\n",
      "Upcoming page is 2\n",
      "Crawling 2 out of 40 pages...\n",
      "Upcoming page is 3\n",
      "Crawling 3 out of 40 pages...\n",
      "Upcoming page is 4\n",
      "\n",
      "Count: 105\n",
      "Refreshing for 1 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "start_url = driver.current_url\n",
    "count = df.shape[0]\n",
    "#go = \"y\"\n",
    "\n",
    "#while go==\"y\":\n",
    "\n",
    "for i in trange(total_page//how_many):\n",
    "    while count<100:\n",
    "        starting_page_num = 1+(3*i)\n",
    "\n",
    "        try:\n",
    "            out = crawl_job_description(starting_page_num, 3, start_url)\n",
    "        except:\n",
    "            if check_http_error() == True:\n",
    "                backup_df = df\n",
    "\n",
    "                if starting_page_num !=1:\n",
    "\n",
    "                    start_url = out[1]\n",
    "                print (\"Take a break...for 3 minuits...\")\n",
    "                time.sleep(180)\n",
    "                #go = input(\"Keep going? (y/n)\")\n",
    "                refresh_link(start_url)\n",
    "                out = crawl_job_description(starting_page_num, 3, start_url)\n",
    "            else: #For simple errors\n",
    "                print (\"refresh due to error...\")\n",
    "                refresh_link(start_url)\n",
    "                driver.implicitly_wait(10)\n",
    "                out = crawl_job_description(starting_page_num, 3, start_url)\n",
    "\n",
    "        start_url = out[1]\n",
    "        df =  pd.concat([df, out[0]])\n",
    "        count = df.shape[0]\n",
    "        print (\"Count: {}\".format(count))\n",
    "        print (\"Refreshing for {} times\".format(i+1))\n",
    "        refresh_link(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"JD_{}.csv\".format(job), index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"JD_{}.xlsx\".format(job), index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Job_Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n              [토스증권팀] Data Analyst</td>\n",
       "      <td>게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Hayoung ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n              Analytics Specialist</td>\n",
       "      <td>Opportunity\\n\\nMightyHive is a new breed of me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n              Data Analytics SA</td>\n",
       "      <td>Description\\n\\nAre you an Analytics, Big Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n              Business Intelligence Analyst,...</td>\n",
       "      <td>The Role\\n\\nPlay a role as a consultant and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n              Data Analyst</td>\n",
       "      <td>매스프레소는 문제 검색, 질문답변, 맞춤형 개념학습 컨텐츠를 제공하는 교육 플랫폼 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n              Data Center Technician (Engineer)</td>\n",
       "      <td>게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            HyunSoo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n              [AIA생명] 고객분석팀 - Data Analyst</td>\n",
       "      <td>게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Flynn Ji...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n              IT Data Analyst-Client Service...</td>\n",
       "      <td>Who We Are Looking For\\n\\nThe client service r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n              서비스디자인팀 리드</td>\n",
       "      <td>게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Hyojin K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n              Computational Linguist</td>\n",
       "      <td>We are currently looking for a Computational L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Position  \\\n",
       "0                \\n              [토스증권팀] Data Analyst   \n",
       "1                \\n              Analytics Specialist   \n",
       "2                   \\n              Data Analytics SA   \n",
       "3   \\n              Business Intelligence Analyst,...   \n",
       "4                        \\n              Data Analyst   \n",
       "..                                                ...   \n",
       "2   \\n              Data Center Technician (Engineer)   \n",
       "3        \\n              [AIA생명] 고객분석팀 - Data Analyst   \n",
       "4   \\n              IT Data Analyst-Client Service...   \n",
       "5                          \\n              서비스디자인팀 리드   \n",
       "6              \\n              Computational Linguist   \n",
       "\n",
       "                                          Job_Details  \n",
       "0   게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Hayoung ...  \n",
       "1   Opportunity\\n\\nMightyHive is a new breed of me...  \n",
       "2   Description\\n\\nAre you an Analytics, Big Data ...  \n",
       "3   The Role\\n\\nPlay a role as a consultant and co...  \n",
       "4   매스프레소는 문제 검색, 질문답변, 맞춤형 개념학습 컨텐츠를 제공하는 교육 플랫폼 ...  \n",
       "..                                                ...  \n",
       "2   게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            HyunSoo ...  \n",
       "3   게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Flynn Ji...  \n",
       "4   Who We Are Looking For\\n\\nThe client service r...  \n",
       "5   게시자:\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Hyojin K...  \n",
       "6   We are currently looking for a Computational L...  \n",
       "\n",
       "[105 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
