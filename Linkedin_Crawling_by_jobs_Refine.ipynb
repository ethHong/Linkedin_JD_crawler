{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains  \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 1: job 별\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_user_agent():\n",
    "    ua = UserAgent()\n",
    "    \n",
    "    working = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\"\n",
    "    working_tail = \"(\" + working.split(\"(\")[-1]\n",
    "    random_head = ua.random.split(\"(\")[0]+\"(\"+ua.random.split(\"(\")[1]\n",
    "    return random_head + working_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_user_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID (Email)sukhyun9673@gmail.com\n",
      "PASSWORDsh96699669\n"
     ]
    }
   ],
   "source": [
    "ID = input(\"ID (Email)\")\n",
    "PASS = input(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HongSukhyun/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: use options instead of chrome_options\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "userAgent = mock_user_agent()\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "#driver =  webdriver.Chrome(\"/Users/HongSukhyun/Desktop/SukhyunHong/20-2/UDS/Course_Recommendation/chromedriver\")\n",
    "#driverpath = os.getcwd()+\"/chromedriver_win\"\n",
    "\n",
    "driverpath = os.getcwd()+\"/chromedriver\"\n",
    "driver =  webdriver.Chrome(driverpath,  chrome_options=options)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "def Login_linkedin(driver, ID, PASS):\n",
    "\n",
    "    url = \"https://www.linkedin.com/\"\n",
    "   \n",
    "    driver.get(url)\n",
    "    #driver.find_element_by_xpath('/html/body/div/main/p/a').click()\n",
    "    \n",
    "    ID = ID\n",
    "    PASS = PASS\n",
    "    \n",
    "    elem = driver.find_element_by_xpath('//*[@id=\"session_key\"]')\n",
    "    elem.send_keys(ID)\n",
    "    elem = driver.find_element_by_xpath('//*[@id=\"session_password\"]')\n",
    "    elem.send_keys(PASS)\n",
    "    \n",
    "   \n",
    "    driver.find_element_by_xpath('/html/body/main/section[1]/div[2]/form/button').click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_link(continue_link):\n",
    "    userAgent = mock_user_agent()\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    driverpath = os.getcwd()+\"/chromedriver\"\n",
    "    driver =  webdriver.Chrome(driverpath,  chrome_options=options)\n",
    "    \n",
    "    Login_linkedin(driver, ID, PASS)\n",
    "    \n",
    "    driver.get(continue_link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Login_linkedin(driver, ID, PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put your job position: operation\n"
     ]
    }
   ],
   "source": [
    "job = input(\"Put your job position: \")\n",
    "region = \"대한민국\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"https://www.linkedin.com/jobs/search/?geoId=105149562&keywords=\"\n",
    "\n",
    "def refine(c):\n",
    "    c_ref = \"-\".join(c.split(\" \")).lower()\n",
    "    return c_ref\n",
    "\n",
    "link = header + refine(job)\n",
    "driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '/html/body/div[7]/div[3]/div[3]/div/div/div/div/section/div/ul'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\") \n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b994f8f71f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"artdeco-pagination__pages artdeco-pagination__pages--number\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"li\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "pages = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_page = int(pages[-1].text.strip()) # # of total page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_source_pages():\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "    driver.implicitly_wait(10)\n",
    "    try:\n",
    "        p = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")\n",
    "    except:\n",
    "        driver.get(driver.current_url)\n",
    "        p = soup.find(\"ul\", {\"class\": \"artdeco-pagination__pages artdeco-pagination__pages--number\"}).find_all(\"li\")\n",
    "    return [p, soup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_jd():\n",
    "    soup = refresh_source_pages()[1]\n",
    "\n",
    "    potision = []\n",
    "    job_details = []\n",
    "\n",
    "    jobs = soup.find_all(\"li\", {\"class\": \"jobs-search-results__list-item occludable-update p0 relative ember-view\"})\n",
    "    jobs_id = [i[\"id\"] for i in jobs]\n",
    "    \n",
    "    for i in jobs_id:\n",
    "        driver.find_element_by_xpath('//*[@id=\"{}\"]'.format(i)).click()\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "        #refresh page source\n",
    "        soup = refresh_source_pages()[1]\n",
    "        \n",
    "        driver.implicitly_wait(10)\n",
    "        \n",
    "        Position =soup.find(\"h2\", {\"class\": \"jobs-details-top-card__job-title t-20 t-black t-normal\"}).text.rstrip()\n",
    "\n",
    "        Job_Details = soup.find(\"div\", {\"id\":\"job-details\"}).text.strip()\n",
    "        potision.append(Position)\n",
    "        job_details.append(Job_Details)\n",
    "\n",
    "    \n",
    "    return pd.DataFrame({\"Position\" : potision, \"Job_Details\": job_details})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many = 3 #안전하게 3번마다 Refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = refresh_source_pages()[0]\n",
    "soup = refresh_source_pages()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Page 넣으면 알아서 크롤링하게 만들기\n",
    "def crawl_job_description(starting_page, how_many, start_url):\n",
    "    driver.get(start_url)\n",
    "    \n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    pages = refresh_source_pages()[0]\n",
    "    soup = refresh_source_pages()[1]\n",
    "    \n",
    "    current = starting_page\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in trange(starting_page-1, how_many+starting_page-1):\n",
    "    \n",
    "        print (\"Crawling {} out of {} pages...\".format(current, total_page))\n",
    "\n",
    "        pages_meta = [j.text.strip().split()[0] for j in pages]    \n",
    "\n",
    "        #Do Crawling#\n",
    "        crawed_page = crawl_jd()\n",
    "        df = pd.concat([df, crawed_page])\n",
    "        current = current+1\n",
    "        #Move page \n",
    "        try:\n",
    "            index_of_next_page = pages_meta.index(str(i+2))\n",
    "        except ValueError:\n",
    "            index_of_next_page = len(pages_meta) -1 - pages_meta[::-1].index('…')\n",
    "\n",
    "        button_aria_label = pages[index_of_next_page].find(\"button\")[\"aria-label\"]\n",
    "\n",
    "        #해당 버튼이 나올때까지 기다려주기\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//*[@aria-label=\"{}\"]'.format(button_aria_label)).click()\n",
    "\n",
    "        except:\n",
    "            driver.get(driver.current_url)\n",
    "            driver.implicitly_wait(10)\n",
    "            button_aria_label = str(int(button_aria_label.split()[0])+1) + \" \" + button_aria_label.split()[1]\n",
    "            driver.find_element_by_xpath('//*[@aria-label=\"{}\"]'.format(button_aria_label)).click()\n",
    "\n",
    "        \n",
    "        driver.implicitly_wait(10)\n",
    "        print (\"Upcoming page is {}\".format(starting_page+i+1))\n",
    "        upcoming = driver.current_url\n",
    "        #Refresh List\n",
    "        try:\n",
    "            pages = refresh_source_pages()[0]\n",
    "        except:\n",
    "            driver.get(driver.current_url)\n",
    "            driver.implicitly_wait(10)\n",
    "        \n",
    "    return (df, upcoming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "df = pd.DataFrame()\n",
    "start_url = driver.current_url\n",
    "\n",
    "for i in trange(total_page//how_many):\n",
    "    starting_page_num = 1+(3*i)\n",
    "    \n",
    "    try:\n",
    "        out = crawl_job_description(starting_page_num, 3, start_url)\n",
    "    except:\n",
    "        refresh_link(start_url)\n",
    "        driver.implicitly_wait(10)\n",
    "        out = crawl_job_description(starting_page_num, 3, start_url)\n",
    "    start_url = out[1]\n",
    "    df =  pd.concat([df, out[0]])\n",
    "    print (\"Refreshing for {} times\".format(i+1))\n",
    "    refresh_link(start_url)\n",
    "    \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"JD_{}.csv\".format(job), index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"JD_{}.xlsx\".format(job), index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
